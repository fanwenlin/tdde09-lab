{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b684dd6",
   "metadata": {},
   "source": [
    "# Statistical bigram model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a8f0f1",
   "metadata": {},
   "source": [
    "In this notebook, we will build our first language model. Large language models such as GPT generate text as a sequence of words. The language model presented here is very small and generates text as a sequence of individual characters (letters). Also, it is not built on a neural network but is based on probabilities defined on pairs of adjacent characters. Because of this, it is known as a **statistical bigram model**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752b28ad",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32d3b6",
   "metadata": {},
   "source": [
    "The task we will address in this notebook is to generate person names. The data for this task comes as a text file containing Swedish first names (*tilltalsnamn*). More specifically, the file lists the most frequent names in Sweden as of Decmber 2022 in decreasing order of frequency. The raw data was obtained from [Statistics Sweden](https://www.scb.se/) and postprocessed by lowercasing each name.\n",
    "\n",
    "We start by opening the file and store its contents in a Python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64664b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"names-train.txt\", encoding=\"utf-8\") as f:\n",
    "    names = [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878a612",
   "metadata": {},
   "source": [
    "Here are the five most frequent names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a9c3b",
   "metadata": {},
   "source": [
    "### üß© Task 1: What‚Äôs in the data?\n",
    "\n",
    "It is important to engage with the data you are working with. One way to do so is to ask questions. Is your own name included in the dataset? If so, is your name frequent or rare? Can you tell from a name whether the person with that name is male or female? Can you tell whether they are immigrants? What would be ethical and unethical uses of systems that *can* tell this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16af3f",
   "metadata": {},
   "source": [
    "## Character-to-index mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1377cd22",
   "metadata": {},
   "source": [
    "It will be convenient to represent characters by numbers. We therefore create a string-to-integer mapping from the characters in the names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6cc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {\"$\": 0}\n",
    "\n",
    "for name in names:\n",
    "    for char in name:\n",
    "        if char not in char2idx:\n",
    "            char2idx[char] = len(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9b820",
   "metadata": {},
   "source": [
    "Note that we reserve a special character `$` with index&nbsp;0. We will use this character to mark the start and the end of a sequence of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4cf23",
   "metadata": {},
   "source": [
    "### ü§Ø What does this code do?\n",
    "\n",
    "Throughout the course, we often present code without detailed explanations. If you need help understanding code, ask your tutor ‚Äì or use an AI tool such as [ChatGPT](https://chatgpt.com/) and [Copilot](https://copilot.microsoft.com/). For example, the following explanation of the above code was generated by ChatGPT&nbsp;5 as a response to the prompt *Explain the following code in four sentences*.\n",
    "\n",
    "> This code builds a dictionary mapping characters to unique integer indices. It starts with a special `$` character assigned to index `0`. Then, for every character in every string inside `names`, it checks if that character is already in the dictionary. If not, it assigns the character the next available index, which is simply the current dictionary length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfa279",
   "metadata": {},
   "source": [
    "### üß© Task 2: A look inside the vocabulary\n",
    "\n",
    "Write code that prints the vocabulary for the names dataset. Would you have expected this vocabulary? Would you expect the same vocabulary for a list of English names?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356723cc",
   "metadata": {},
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969b408",
   "metadata": {},
   "source": [
    "As already mentioned, our model is based on pairs of consecutive characters (letters). Such pairs are called **bigrams**. For example, the character bigrams in the name `anna` are `an`, `nn`, and `na`. If we also include the start and end marker `$`, the bigrams are `$a`, `an`, `nn`, `na`, and `a$`.\n",
    "\n",
    "The code in the following cell generates all character bigrams from a given iterable of names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f255c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(names):\n",
    "    for name in names:\n",
    "        for x, y in zip(\"$\" + name, name + \"$\"):\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ca4a57",
   "metadata": {},
   "source": [
    "For example, here are the bigrams extracted from the first two names in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8433fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[b for b in bigrams(names[:2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039626f7",
   "metadata": {},
   "source": [
    "### ü§Ø Generator functions\n",
    "\n",
    "Note that `bigrams()` is a **generator function**: It does not `return` a list of all bigrams, it `yield`s all bigrams. This is more efficient in terms of memory usage, especially when dealing with the larger datasets we will encounter in this course. If you have not worked with generators and iterators before, now is a good time to read up on them. [More information about generators](https://wiki.python.org/moin/Generators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075264e3",
   "metadata": {},
   "source": [
    "## Estimating bigram probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b767e85",
   "metadata": {},
   "source": [
    "How does a language model generate text? Intuitively, we can imagine rolling a multi-sided dice whose sides correspond to the elements of the vocabulary. Whereas standard dice are fair (all sides land face up with the same uniform probability), the dice rolled by language models are weighted.\n",
    "\n",
    "**The basic idea behind a bigram model is to let the probability of the next element in a generated sequence depend on the previous element (and only on that one).**\n",
    "\n",
    "To estimate the probabilities of a bigram model, we start by counting how often each bigram occurs in the dataset. We can keep track of these counts by arranging them in a matrix&nbsp;$M$ that has as many rows and as many columns as there are characters in our vocabulary&nbsp;$V$. More formally, let $V = \\{c_0, \\dots, c_{n-1}\\}$ where each $c_i$ is a character. Then the matrix entry $M_{ij}$ should be the count of the character bigram $c_ic_j$ in our list of names. To compute this matrix, we can use the code in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e27cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a counts matrix with all zeros\n",
    "counts = torch.zeros(len(char2idx), len(char2idx))\n",
    "\n",
    "# Update the counts based on the bigrams\n",
    "for x, y in bigrams(names):\n",
    "    counts[char2idx[x], char2idx[y]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b82e3ed",
   "metadata": {},
   "source": [
    "Note that we represent matrices as *tensors* from the [PyTorch library](https://pytorch.org/). You will learn more about this library later in the course.\n",
    "\n",
    "Now that we have the bigram counts, we are ready to define our bigram model. This model is essentially a conditional probability distribution over all possible next characters, given the immediately preceding character. Formally, using the notation introduced above, the model consists of probabilities of the form $p(c_j\\,|\\,c_i)$, which quantify the probability of character&nbsp;$c_j$ given that the preceding character is&nbsp;$c_i$. To compute these probabilities, we divide each bigram count $M_{ij}$ by the sum along the $i$-th row of&nbsp;$M$. We can accomplish this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75cb80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = counts / counts.sum(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe749cf",
   "metadata": {},
   "source": [
    "### üß© Task 3: Inspecting the model\n",
    "\n",
    "According to this model, which letter is most likely to come after the letter `c`? Which letter is most likely to start or end a name? Can you give an example of a name that is impossible according to the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d9ba1",
   "metadata": {},
   "source": [
    "## Generating text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b651c2a",
   "metadata": {},
   "source": [
    "Now that we have estimated the probabilities of our bigram model, we can generate text. To do so, we repeatedly ‚Äúroll a dice‚Äù by sampling from the next-character distribution, conditioning on the previous character. Equivalently, we can think of sampling from a collection of categorical distributions over the next character, one distribution per previous character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the inverse of the character-to-index mapping\n",
    "idx2char = {i: c for c, i in char2idx.items()}\n",
    "\n",
    "# Generate 5 samples\n",
    "for _ in range(5):\n",
    "    # We begin with the start-of-sequence marker\n",
    "    generated = \"$\"\n",
    "\n",
    "    while True:\n",
    "        # Look up the integer index of the previous character\n",
    "        previous_idx = char2idx[generated[-1]]\n",
    "\n",
    "        # Get the relevant probability distribution\n",
    "        probs = model[previous_idx]\n",
    "\n",
    "        # Sample an index from the distribution\n",
    "        next_idx = int(torch.multinomial(probs, num_samples=1).item())\n",
    "\n",
    "        # Get the corresponding character\n",
    "        next_char = idx2char[next_idx]\n",
    "\n",
    "        # Break if the model generates the end-of-sequence marker\n",
    "        if next_char == \"$\":\n",
    "            break\n",
    "\n",
    "        # Otherwise, add the next character to the output\n",
    "        generated = generated + next_char\n",
    "\n",
    "    # Print the generated output (without the start-of-sequence marker)\n",
    "    print(generated[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacff22f",
   "metadata": {},
   "source": [
    "As we can see, the strings generated by our bigram model only vaguely resemble actual names. This should not really surprise us: after all, each next character is generated by only looking at the immediately preceding character, which is too short a context to model many important aspects of names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0423a52",
   "metadata": {},
   "source": [
    "### üß© Task 4: Probability of a name\n",
    "\n",
    "What is the probability of our model generating your name? What is the probability of it generating the single-letter ‚Äúname‚Äù `s`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c916c9f",
   "metadata": {},
   "source": [
    "## Evaluating language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f4996",
   "metadata": {},
   "source": [
    "Language models are commonly evaluated by computing their **perplexity**. Intuitively, this measures how ‚Äúsurprised‚Äù the model is about a given text. The higher the perplexity, the lower the probability which the model assigns to the text. To obtain the perplexity of a model on a given text, we first compute the average negative log-likelihood $-\\log p(c_j|c_i)$ that the model assigns to each true next character $c_j$ given its predecessor $c_i$. Then, we exponentiate this average.\n",
    "\n",
    "(In this course, the notation $\\log x$ refers to the natural (base $e$) logarithm of&nbsp;$x$, which in other contexts is also written as $\\ln x$ or $\\log_e x$.)\n",
    "\n",
    "The code in the next cell computes the perplexity of our bigram model on a held-out list of names. This list was sourced from [Wikipedia](https://en.wikipedia.org/w/index.php?title=Category:Swedish_given_names) and lightly edited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed97cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Read the test data\n",
    "with open(\"names-test.txt\", encoding=\"utf-8\") as f:\n",
    "    test_names = [line.rstrip() for line in f]\n",
    "\n",
    "# Collect the negative log likelihoods\n",
    "nlls = []\n",
    "for prev_char, next_char in bigrams(test_names):\n",
    "    nlls.append(-math.log(model[char2idx[prev_char], char2idx[next_char]]))\n",
    "\n",
    "# Compute the perplexity\n",
    "ppl = math.exp(sum(nlls) / len(nlls))\n",
    "\n",
    "# Print the perplexity\n",
    "print(f\"{ppl:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f6c78b",
   "metadata": {},
   "source": [
    "### üß© Task 5: Upper bound on perplexity\n",
    "\n",
    "The perplexity of our bigram model can be interpreted as the average number of characters the model must choose from when trying to ‚Äúguess‚Äù the reference text. The lowest possible perplexity value is&nbsp;1, which corresponds to the case where each next character is completely certain and no guessing is necessary. What is a reasonable *upper bound* on the perplexity of our bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb2241",
   "metadata": {},
   "source": [
    "That‚Äôs all folks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
